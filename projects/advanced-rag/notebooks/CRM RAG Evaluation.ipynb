{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRM RAG Evaluation\n",
    "\n",
    "Author: Theodore Mui <theodoremui@gmail.com>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "from IPython.display import clear_output\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from llama_index.core import (\n",
    "    PromptTemplate,\n",
    "    Settings,\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.01, timeout=240)\n",
    "embedding = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding\n",
    "\n",
    "hello_embedding = embedding.get_text_embedding(\"hello\")\n",
    "EMBEDDING_DIM = len(hello_embedding)\n",
    "print(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. File Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_cloud_services import LlamaParse\n",
    "\n",
    "notebook_dir = Path().absolute()\n",
    "\n",
    "crm_folder = str(notebook_dir / \"..\" / \"data\" / \"crm-docs\")\n",
    "LLAMA_CLOUD_API_KEY = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "len(LLAMA_CLOUD_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to read Salesforce SEC 10K filing: [10K](https://investor.salesforce.com/financials/sec-filings/sec-filings-details/default.aspx?FilingId=18259118)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 8.09 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "parser = LlamaParse(\n",
    "    num_workers=6,\n",
    "    result_type=\"markdown\",\n",
    "    api_key=LLAMA_CLOUD_API_KEY\n",
    ")\n",
    "file_extractor = {\".pdf\": parser}\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_dir=crm_folder,\n",
    "    file_extractor=file_extractor\n",
    ").load_data(num_workers=10)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('# Table of Contents\\n'\n",
      " '\\n'\n",
      " '# UNITED STATES\\n'\n",
      " '\\n'\n",
      " '# SECURITIES AND EXCHANGE COMMISSION\\n'\n",
      " '\\n'\n",
      " '# Washington, D.C. 20549')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(documents[0].text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'creation_date': '2025-04-05',\n",
      " 'file_name': 'salesforce-fy24-10k.pdf',\n",
      " 'file_path': 'c:\\\\Users\\\\theod\\\\OneDrive\\\\dev\\\\research2025\\\\projects\\\\advanced-rag\\\\notebooks\\\\..\\\\data\\\\crm-docs\\\\salesforce-fy24-10k.pdf',\n",
      " 'file_size': 1516703,\n",
      " 'file_type': 'application/pdf',\n",
      " 'last_modified_date': '2025-04-05'}\n"
     ]
    }
   ],
   "source": [
    "pprint(documents[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading Evaluation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.testset import Testset\n",
    "import json\n",
    "\n",
    "folder_path = notebook_dir / \"..\" / \"data\" / \"crm-eval\"\n",
    "if not folder_path.exists():\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sdg_folder = str(folder_path)\n",
    "\n",
    "# Helper function to read JSONL files with UTF-8 encoding\n",
    "def read_jsonl_with_utf8(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "# Read the files with UTF-8 encoding\n",
    "single_hop_specific_testset = Testset.from_list(read_jsonl_with_utf8(f\"{sdg_folder}/single_hop_specific_testset.jsonl\"))\n",
    "multi_hop_specific_testset = Testset.from_list(read_jsonl_with_utf8(f\"{sdg_folder}/multi_hop_specific_testset.jsonl\"))\n",
    "multi_hop_abstract_testset = Testset.from_list(read_jsonl_with_utf8(f\"{sdg_folder}/multi_hop_abstract_testset.jsonl\"))\n",
    "\n",
    "single_hop_specific_list = single_hop_specific_testset.to_list()\n",
    "multi_hop_specific_list = multi_hop_specific_testset.to_list()\n",
    "multi_hop_abstract_list = multi_hop_abstract_testset.to_list()\n",
    "\n",
    "len(single_hop_specific_list), len(multi_hop_specific_list), len(multi_hop_abstract_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_input': 'How does Salesforce enhance customer relationship management for businesses?',\n",
       " 'reference_contexts': ['Overview Salesforce, Inc. (“Salesforce,” the “Company,” “we” or “our”) is a global leader in customer relationship management (“CRM”) technology, enabling companies of every size and industry to connect with their customers through the power of data, artificial intelligence (“AI”), CRM and trust. Founded in 1999, we bring humans together with AI agents to drive customer success on one deeply unified platform. Our AI-powered Salesforce Platform unites our offerings — spanning sales, service, marketing, commerce, collaboration, integration, AI, analytics, automation, industries and more — by connecting customer data across systems, applications and devices to create a complete view of customers. With this single source of customer truth, teams can be more responsive, productive and efficient and deliver intelligent, personalized and automated experiences across every channel. With Agentforce, the agentic layer of the Salesforce Platform, our customers can build and augment their teams with an always-on digital labor force, deploying autonomous AI agents across business functions that aim to increase productivity, lower costs and drive operational efficiencies. Our service offerings are designed to be flexible, scalable and easy to use. They can generally be configured easily, deployed rapidly and integrated with other platforms and enterprise applications. We sell to businesses worldwide, primarily on a subscription basis, through our direct sales efforts and also indirectly through partners. In addition, we enable third parties to use our platform and developer tools to create additional functionality and new applications that run on our platform, which are sold separately from, or in conjunction with, our service offerings. Salesforce is committed to a core set of values: trust, customer success, innovation, equality and sustainability – all of which are grounded in legal and regulatory frameworks that guide and inform our business. Foremost among these is trust, which is paramount and the foundation for everything we do, and is also firmly rooted in compliance with applicable laws governing security, privacy, data protection and operational integrity. Our customers expect to trust and rely on our technology to meet the high enterprise-grade standards of security, privacy, performance, legal compliance and availability at scale. Customer success is at the core of our business, and we align the entire company around our customers’ needs, promoting their success, showing our value and upholding applicable laws, contractual obligations and industry regulations and standards. Innovation is fundamental to our mission, empowering and enabling our customers to stay ahead in their industries and driving technological advancements in line with evolving laws, standards and guidelines. Equality is a legal and ethical mandate and a core tenet that informs how we operate. Our commitment to equal opportunity is anchored in applicable laws, statutes, regulations and principles. We value the equality of every individual at our company and in our communities and are dedicated to fostering a workplace that complies with these protections, creating an inclusive culture where every individual feels seen, heard and valued. Finally, we are committed to creating a more sustainable and nature-positive future for all. Our products and services help our customers meet their own sustainability and compliance priorities, guided by applicable environmental and sustainability-related laws, corporate social responsibility frameworks and legal requirements. By grounding our values in legal and regulatory principles, we reinforce our opportunity and responsibility to uphold high integrity and robust ethical standards, ensuring that trust, fairness, and compliance remain central to everything we seek to do. We believe that our values, grounded in legal and regulatory frameworks, create value, and the business of business is to make the world a better place for all of our stakeholders, including stockholders, customers, employees, partners, the planet and the communities in which we work and live. Salesforce is committed to giving back to our communities, helping businesses grow while protecting the environment for future generations, and transparent environmental, social and governance disclosures. We believe we have a broad responsibility to society, and we aspire to create a framework for the ethical and humane use of technology that not only drives the success of our customers, but also upholds the basic human rights of every individual. #'],\n",
       " 'reference': 'Salesforce enhances customer relationship management by enabling companies to connect with their customers through data, artificial intelligence, and a unified platform. It provides a complete view of customers by connecting customer data across systems, applications, and devices, allowing teams to be more responsive, productive, and efficient. Additionally, Salesforce offers flexible and scalable service offerings that can be easily configured, deployed, and integrated with other platforms, promoting customer success and innovation.',\n",
       " 'synthesizer_name': 'single_hop_specific'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_hop_specific_list[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Constructing RAG Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Creating Text Chunkers (\"Node Parsers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import (\n",
    "    TokenTextSplitter,\n",
    "    MarkdownNodeParser, MarkdownElementNodeParser,\n",
    "    SemanticSplitterNodeParser,\n",
    "    SentenceSplitter, \n",
    "    SentenceWindowNodeParser,\n",
    ")\n",
    "\n",
    "from llama_index.core.retrievers import RecursiveRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from typing import Callable, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_text_splitter = TokenTextSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=32,\n",
    ")\n",
    "\n",
    "sentence_splitter = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=32,\n",
    ")\n",
    "\n",
    "md_node_parser = MarkdownNodeParser.from_defaults(\n",
    "    include_metadata=True,\n",
    "    include_prev_next_rel=True,\n",
    ")\n",
    "\n",
    "mde_node_parser = MarkdownElementNodeParser(\n",
    "    llm=llm,\n",
    "    num_workers=10,\n",
    ").from_defaults()\n",
    "\n",
    "semantic_splitter = SemanticSplitterNodeParser(\n",
    "    breakpoint_percentile_threshold=95,\n",
    "    embed_model=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Creating Indices & Query Engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 155/155 [00:05<00:00, 26.27it/s]\n",
      "Generating embeddings: 100%|██████████| 183/183 [00:12<00:00, 14.93it/s]\n"
     ]
    }
   ],
   "source": [
    "token_text_splitter_index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    "    show_progress=True,\n",
    "    node_parser=token_text_splitter,\n",
    ")\n",
    "token_text_splitter_query_engine = token_text_splitter_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 155/155 [00:00<00:00, 628.86it/s]\n",
      "Generating embeddings: 100%|██████████| 183/183 [00:21<00:00,  8.59it/s]\n"
     ]
    }
   ],
   "source": [
    "sentence_splitter_index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    "    show_progress=True,\n",
    "    node_parser=sentence_splitter,\n",
    "    num_workers=10,\n",
    ")\n",
    "sentence_splitter_query_engine = sentence_splitter_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 568/568 [00:27<00:00, 20.67it/s]\n"
     ]
    }
   ],
   "source": [
    "md_nodes = md_node_parser.get_nodes_from_documents(documents)\n",
    "md_index = VectorStoreIndex(\n",
    "    nodes=md_nodes,\n",
    "    show_progress=True,\n",
    "    num_workers=10,\n",
    ")\n",
    "md_query_engine = md_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "5it [00:00, ?it/s]\n",
      "3it [00:00, 3043.76it/s]\n",
      "3it [00:00, ?it/s]\n",
      "2it [00:00, 556.38it/s]\n",
      "1it [00:00, 659.38it/s]\n",
      "2it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "2it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, 973.38it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "3it [00:00, ?it/s]\n",
      "1it [00:00, 662.29it/s]\n",
      "1it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "1it [00:00, 659.38it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "2it [00:00, ?it/s]\n",
      "4it [00:00, 4006.98it/s]\n",
      "3it [00:00, 5711.72it/s]\n",
      "2it [00:00, 1318.96it/s]\n",
      "1it [00:00, 984.35it/s]\n",
      "3it [00:00, ?it/s]\n",
      "2it [00:00, ?it/s]\n",
      "2it [00:00, 1974.72it/s]\n",
      "2it [00:00, ?it/s]\n",
      "3it [00:00, ?it/s]\n",
      "2it [00:00, ?it/s]\n",
      "2it [00:00, ?it/s]\n",
      "3it [00:00, ?it/s]\n",
      "3it [00:00, 1978.76it/s]\n",
      "3it [00:00, 4686.37it/s]\n",
      "1it [00:00, 957.82it/s]\n",
      "2it [00:00, ?it/s]\n",
      "1it [00:00, 655.87it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "1it [00:00, 661.25it/s]\n",
      "2it [00:00, ?it/s]\n",
      "1it [00:00, 999.36it/s]\n",
      "1it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, 985.50it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, 660.10it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, 961.11it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "1it [00:00, 999.12it/s]\n",
      "1it [00:00, 978.38it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of initial mde nodes: 426\n",
      "Number of text nodes: 244\n",
      "Number of table nodes: 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 335/335 [01:08<00:00,  4.92it/s]\n"
     ]
    }
   ],
   "source": [
    "mde_nodes = mde_node_parser.get_nodes_from_documents(documents=documents, verbose=False)\n",
    "\n",
    "print(f\"Number of initial mde nodes: {len(mde_nodes)}\")\n",
    "\n",
    "# Get text nodes and object (table) nodes\n",
    "base_nodes, objects = mde_node_parser.get_nodes_and_objects(nodes=mde_nodes)\n",
    "\n",
    "print(f\"Number of text nodes: {len(base_nodes)}\")\n",
    "print(f\"Number of table nodes: {len(objects)}\")\n",
    "for i in range(len(objects)):\n",
    "    objects[i].text = objects[i].text[:]\n",
    "\n",
    "mde_index = VectorStoreIndex(\n",
    "    nodes=base_nodes + objects,\n",
    "    show_progress=True,\n",
    "    num_workers=10,\n",
    ")\n",
    "mde_query_engine = mde_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 358/358 [00:33<00:00, 10.56it/s]\n"
     ]
    }
   ],
   "source": [
    "semantic_nodes = semantic_splitter.get_nodes_from_documents(documents=documents)\n",
    "\n",
    "semantic_index = VectorStoreIndex(\n",
    "    nodes=semantic_nodes,\n",
    "    show_progress=True,\n",
    "    num_workers=10,\n",
    ")\n",
    "semantic_query_engine = semantic_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating RAG engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate, EvaluationDataset\n",
    "from ragas.llms import LlamaIndexLLMWrapper\n",
    "from ragas.metrics import (\n",
    "    AnswerAccuracy,\n",
    "    AnswerCorrectness,\n",
    "    AnswerRelevancy, \n",
    "    ContextRelevance,\n",
    "    ContextPrecision, \n",
    "    ContextRecall,\n",
    "    Faithfulness, \n",
    "    FactualCorrectness, \n",
    "    ResponseGroundedness, \n",
    "    ResponseRelevancy\n",
    ")\n",
    "\n",
    "evaluator_llm = LlamaIndexLLMWrapper(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_queries(query_engine, test_cases_list):\n",
    "\n",
    "    # Step 1: user the query_engine to answer each `user_input`\n",
    "    total_queries = len(test_cases_list)\n",
    "    for test_case in tqdm(test_cases_list, \n",
    "                          desc=\"\\tResponding to queries\",\n",
    "                          total=total_queries,\n",
    "                          unit=\"query\",\n",
    "                          position=0,\n",
    "                          leave=True):\n",
    "        response = query_engine.query(test_case[\"user_input\"])\n",
    "        test_case[\"response\"] = str(response.response)\n",
    "        test_case[\"retrieved_contexts\"] = [node.text for node in response.source_nodes]\n",
    "    \n",
    "    # Step 2: evaluate how good the answers are relative to the\n",
    "    #         `reference` in the test_cases_list (eval dataset)\n",
    "    eval_dataset = EvaluationDataset.from_list(test_cases_list)\n",
    "    eval_result = evaluate(\n",
    "        dataset=eval_dataset,\n",
    "        llm=evaluator_llm,\n",
    "        metrics=[\n",
    "            AnswerCorrectness(),\n",
    "            AnswerRelevancy(), \n",
    "            ContextPrecision(), \n",
    "            ContextRecall(),\n",
    "            Faithfulness(), \n",
    "            FactualCorrectness(), \n",
    "            ResponseRelevancy(),\n",
    "        ],\n",
    "        show_progress=False,\n",
    "    )\n",
    "    # only select the newly added metrics which are of dtype float64\n",
    "    return eval_result.to_pandas().select_dtypes(include=['float64'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Single Hop Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example using `token_text_splitter_query_engine`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Responding to queries: 100%|██████████| 100/100 [04:47<00:00,  2.88s/query]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5.23 s\n",
      "Wall time: 4min 47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "total_queries = len(single_hop_specific_list)\n",
    "for test_case in tqdm(single_hop_specific_list, \n",
    "                      desc=\"Responding to queries\",\n",
    "                      total=total_queries,\n",
    "                      unit=\"query\",\n",
    "                      position=0,\n",
    "                      leave=True):\n",
    "    token_text_splitter_response = token_text_splitter_query_engine.query(test_case[\"user_input\"])\n",
    "    test_case[\"response\"] = str(token_text_splitter_response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'user_input': 'What CRM do Salesforce do?',\n",
       "  'reference_contexts': ['Overview Salesforce, Inc. (“Salesforce,” the “Company,” “we” or “our”) is a global leader in customer relationship management (“CRM”) technology, enabling companies of every size and industry to connect with their customers through the power of data, artificial intelligence (“AI”), CRM and trust. Founded in 1999, we bring humans together with AI agents to drive customer success on one deeply unified platform. Our AI-powered Salesforce Platform unites our offerings — spanning sales, service, marketing, commerce, collaboration, integration, AI, analytics, automation, industries and more — by connecting customer data across systems, applications and devices to create a complete view of customers. With this single source of customer truth, teams can be more responsive, productive and efficient and deliver intelligent, personalized and automated experiences across every channel. With Agentforce, the agentic layer of the Salesforce Platform, our customers can build and augment their teams with an always-on digital labor force, deploying autonomous AI agents across business functions that aim to increase productivity, lower costs and drive operational efficiencies. Our service offerings are designed to be flexible, scalable and easy to use. They can generally be configured easily, deployed rapidly and integrated with other platforms and enterprise applications. We sell to businesses worldwide, primarily on a subscription basis, through our direct sales efforts and also indirectly through partners. In addition, we enable third parties to use our platform and developer tools to create additional functionality and new applications that run on our platform, which are sold separately from, or in conjunction with, our service offerings. Salesforce is committed to a core set of values: trust, customer success, innovation, equality and sustainability – all of which are grounded in legal and regulatory frameworks that guide and inform our business. Foremost among these is trust, which is paramount and the foundation for everything we do, and is also firmly rooted in compliance with applicable laws governing security, privacy, data protection and operational integrity. Our customers expect to trust and rely on our technology to meet the high enterprise-grade standards of security, privacy, performance, legal compliance and availability at scale. Customer success is at the core of our business, and we align the entire company around our customers’ needs, promoting their success, showing our value and upholding applicable laws, contractual obligations and industry regulations and standards. Innovation is fundamental to our mission, empowering and enabling our customers to stay ahead in their industries and driving technological advancements in line with evolving laws, standards and guidelines. Equality is a legal and ethical mandate and a core tenet that informs how we operate. Our commitment to equal opportunity is anchored in applicable laws, statutes, regulations and principles. We value the equality of every individual at our company and in our communities and are dedicated to fostering a workplace that complies with these protections, creating an inclusive culture where every individual feels seen, heard and valued. Finally, we are committed to creating a more sustainable and nature-positive future for all. Our products and services help our customers meet their own sustainability and compliance priorities, guided by applicable environmental and sustainability-related laws, corporate social responsibility frameworks and legal requirements. By grounding our values in legal and regulatory principles, we reinforce our opportunity and responsibility to uphold high integrity and robust ethical standards, ensuring that trust, fairness, and compliance remain central to everything we seek to do. We believe that our values, grounded in legal and regulatory frameworks, create value, and the business of business is to make the world a better place for all of our stakeholders, including stockholders, customers, employees, partners, the planet and the communities in which we work and live. Salesforce is committed to giving back to our communities, helping businesses grow while protecting the environment for future generations, and transparent environmental, social and governance disclosures. We believe we have a broad responsibility to society, and we aspire to create a framework for the ethical and humane use of technology that not only drives the success of our customers, but also upholds the basic human rights of every individual. #'],\n",
       "  'reference': 'Salesforce is a global leader in customer relationship management (CRM) technology, enabling companies to connect with their customers through data, artificial intelligence, and trust.',\n",
       "  'synthesizer_name': 'single_hop_specific',\n",
       "  'response': 'Salesforce provides its own Customer Relationship Management (CRM) solutions.'},\n",
       " 100)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_hop_specific_list[0], len(single_hop_specific_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a mapping for all query engines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_mapping = {\n",
    "    \"token\": token_text_splitter_query_engine,\n",
    "    \"sentence\": sentence_splitter_query_engine,\n",
    "    \"markdown\": md_query_engine,\n",
    "    \"markdown_element\": mde_query_engine,\n",
    "    \"semantic\": semantic_query_engine,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----token-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tResponding to queries: 100%|██████████| 1/1 [00:07<00:00,  7.74s/query]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "NUM_RECORDS = 1\n",
    "\n",
    "eval_results = {}\n",
    "for query_engine_name, query_engine in query_engine_mapping.items():\n",
    "    print(f\"-----{query_engine_name}-----\")\n",
    "    single_hop = single_hop_specific_list.copy()[:NUM_RECORDS]\n",
    "    eval_results[\"single_hop_\" + query_engine_name]         = process_queries(query_engine, single_hop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Restoring Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "os.makedirs('results', exist_ok=True)\n",
    "serializable_results = {}\n",
    "for model_name, df in eval_results.items():\n",
    "    serializable_results[model_name] = df.to_dict(orient='records')\n",
    "with open('results/eval_results_basic.json', 'w') as f:\n",
    "    json.dump(serializable_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/eval_results_basic.json', 'r') as f:\n",
    "    loaded_results = json.load(f)\n",
    "\n",
    "eval_results_loaded = {\n",
    "    model_name: pd.DataFrame(data) \n",
    "    for model_name, data in loaded_results.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "def quick_compare(original, loaded):\n",
    "    try:\n",
    "        for model in original.keys():\n",
    "            assert_frame_equal(original[model], loaded[model])\n",
    "        print(\"All DataFrames are identical!\")\n",
    "        return True\n",
    "    except AssertionError as e:\n",
    "        print(f\"Differences found: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All DataFrames are identical!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quick_compare(eval_results, eval_results_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'single_hop_markdown':    answer_correctness  answer_relevancy  context_precision  context_recall  \\\n",
      "0                 NaN               NaN                NaN             0.5   \n",
      "1                 NaN               NaN                NaN             NaN   \n",
      "\n",
      "   faithfulness  factual_correctness(mode=f1)  \n",
      "0           NaN                           NaN  \n",
      "1           NaN                           NaN  ,\n",
      " 'single_hop_markdown_element':    answer_correctness  answer_relevancy  context_precision  context_recall  \\\n",
      "0                 NaN          0.796463                1.0             1.0   \n",
      "1                 NaN          0.930385                NaN             NaN   \n",
      "\n",
      "   faithfulness  factual_correctness(mode=f1)  \n",
      "0           NaN                           NaN  \n",
      "1           NaN                           NaN  ,\n",
      " 'single_hop_semantic':    answer_correctness  answer_relevancy  context_precision  context_recall  \\\n",
      "0                 NaN          0.796462                NaN             NaN   \n",
      "1                 NaN          0.930385                NaN             1.0   \n",
      "\n",
      "   faithfulness  factual_correctness(mode=f1)  \n",
      "0           NaN                           NaN  \n",
      "1           NaN                           NaN  ,\n",
      " 'single_hop_sentence':    answer_correctness  answer_relevancy  context_precision  context_recall  \\\n",
      "0                 NaN          0.953154                NaN             0.5   \n",
      "1                 NaN          0.930385                NaN             NaN   \n",
      "\n",
      "   faithfulness  factual_correctness(mode=f1)  \n",
      "0           NaN                           NaN  \n",
      "1           NaN                           NaN  ,\n",
      " 'single_hop_token':    answer_correctness  answer_relevancy  context_precision  context_recall  \\\n",
      "0                 NaN               NaN                NaN             NaN   \n",
      "1                 NaN               NaN                NaN            0.75   \n",
      "\n",
      "   faithfulness  factual_correctness(mode=f1)  \n",
      "0           NaN                           NaN  \n",
      "1           NaN                           NaN  }\n"
     ]
    }
   ],
   "source": [
    "pprint(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Evlauation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_df(eval_results: dict) -> pd.DataFrame:\n",
    "    metrics_dict = {}\n",
    "    for model_name, df in eval_results.items():\n",
    "        # Extract just the metric values (first row since each DataFrame has only one row)\n",
    "        metrics_dict[model_name] = {\n",
    "            'answer_correctness': df['answer_correctness'].mean(),\n",
    "            'answer_relevancy': df['answer_relevancy'].mean(),\n",
    "            'context_precision': df['context_precision'].mean(),\n",
    "            'context_recall': df['context_recall'].mean(),\n",
    "            'faithfulness': df['faithfulness'].mean(),\n",
    "            'factual_correctness(mode=f1)': df['factual_correctness(mode=f1)'].mean()\n",
    "        }\n",
    "\n",
    "    # Now create the DataFrame\n",
    "    df = pd.DataFrame.from_dict(metrics_dict, orient='index')\n",
    "    df = df.round(3)\n",
    "    return df\n",
    "\n",
    "df = get_metrics_df(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>single_hop_token</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single_hop_sentence</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single_hop_markdown</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single_hop_markdown_element</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.863</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single_hop_semantic</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.863</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             answer_correctness  answer_relevancy  \\\n",
       "single_hop_token                            NaN               NaN   \n",
       "single_hop_sentence                         NaN             0.942   \n",
       "single_hop_markdown                         NaN               NaN   \n",
       "single_hop_markdown_element                 NaN             0.863   \n",
       "single_hop_semantic                         NaN             0.863   \n",
       "\n",
       "                             context_precision  context_recall  faithfulness  \\\n",
       "single_hop_token                           NaN            0.75           NaN   \n",
       "single_hop_sentence                        NaN            0.50           NaN   \n",
       "single_hop_markdown                        NaN            0.50           NaN   \n",
       "single_hop_markdown_element                1.0            1.00           NaN   \n",
       "single_hop_semantic                        NaN            1.00           NaN   \n",
       "\n",
       "                             factual_correctness(mode=f1)  \n",
       "single_hop_token                                      NaN  \n",
       "single_hop_sentence                                   NaN  \n",
       "single_hop_markdown                                   NaN  \n",
       "single_hop_markdown_element                           NaN  \n",
       "single_hop_semantic                                   NaN  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
