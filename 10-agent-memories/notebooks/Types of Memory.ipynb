{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Memories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.vector_stores import SimpleVectorStore\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4.1-mini\")\n",
    "embedding = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "Settings.embed_model = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Usfeful for getting the weather for a given location.\"\"\"\n",
    "    return f\"The weather at {location} is very nice with not much rain.\"\n",
    "\n",
    "tool = FunctionTool.from_defaults(\n",
    "    get_weather,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "from llama_index.core.memory import Memory, InsertMethod\n",
    "from llama_index.core.memory import (\n",
    "    StaticMemoryBlock,\n",
    "    FactExtractionMemoryBlock,\n",
    "    VectorMemoryBlock,\n",
    ")\n",
    "\n",
    "static_memory_block = StaticMemoryBlock(\n",
    "    name=\"core_info\",\n",
    "    priority=0,\n",
    "    static_content=\"Name: John Doe, Age: 30, Location: New York, Occupation: Software Engineer\"\n",
    ")\n",
    "facts_block = FactExtractionMemoryBlock(\n",
    "    name=\"facts\",\n",
    "    priority=1,\n",
    "    llm=llm,\n",
    "    max_facts=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'vector_memory' already exists\n",
      "VectorMemoryBlock created successfully with Qdrant!\n"
     ]
    }
   ],
   "source": [
    "# First, start Qdrant locally using Docker (run this in terminal):\n",
    "# docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "def setup_qdrant_with_collection(collection_name=\"vector_memory\", vector_size=1536):\n",
    "    \"\"\"Setup Qdrant client and create collection if it doesn't exist\"\"\"\n",
    "    try:\n",
    "        # Try to connect to local Qdrant\n",
    "        client = QdrantClient(\n",
    "            url=\"http://localhost:6333\",\n",
    "        )\n",
    "        \n",
    "        # Check if collection exists, if not create it\n",
    "        try:\n",
    "            collection_info = client.get_collection(collection_name)\n",
    "            print(f\"Collection '{collection_name}' already exists\")\n",
    "        except Exception: # Collection doesn't exist, create it\n",
    "            client.create_collection(\n",
    "                collection_name=collection_name,\n",
    "                vectors_config=models.VectorParams(\n",
    "                    size=vector_size,  # This should match your embedding dimension\n",
    "                    distance=models.Distance.COSINE\n",
    "                )\n",
    "            )\n",
    "            print(f\"Created collection '{collection_name}'\")\n",
    "        \n",
    "        # Create the vector store\n",
    "        vector_store = QdrantVectorStore(\n",
    "            collection_name=collection_name,\n",
    "            client=client,\n",
    "            prefer_grpc=False,  # Use HTTP instead of gRPC for simplicity\n",
    "        )\n",
    "        \n",
    "        return vector_store, client\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Qdrant: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Setup Qdrant\n",
    "vector_store, qdrant_client = setup_qdrant_with_collection()\n",
    "\n",
    "if vector_store is not None:\n",
    "    vector_memory_block = VectorMemoryBlock(\n",
    "        name=\"vector_memory\",\n",
    "        vector_store=vector_store,\n",
    "        embed_model=embed_model,\n",
    "        priority=2,\n",
    "    )\n",
    "    print(\"VectorMemoryBlock created successfully with Qdrant!\")\n",
    "else:\n",
    "    print(\"Skipping VectorMemoryBlock creation due to Qdrant connection issues\")\n",
    "    vector_memory_block = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've setup three memory blocks:\n",
    "\n",
    "- `core_info`: A static memory block that stores some core information about the user. The static content can either be a string or a list of `ContentBlock` objects like `TextBlock`, ImageBlock, etc. This information will always be inserted into the memory.\n",
    "- `extracted_info`: An extracted memory block that will extract information from the chat history. Here we've passed in the llm to use to extarct facts from the flushed chat history, and set the `max_facts` to 50. If the number of extracted facts exceeds this limit, the max\\_`facts will be automatically summarized and reduced to leave room for new information.\n",
    "- `vector_memory`: A vector memory block that will store and retrieve batches of chat messages from a vector database. Each batch is a list of the flushed chat messages. Here we've passed in the `vector_store` and `embed_model` to use to store and retrieve the chat messages.\n",
    "\n",
    "You'll also notice that we've set the `priority` for each block. This is used to determine the handling when the memory blocks content (i.e. long-term memory) + short-term memory exceeds the token limit on the Memory object.\n",
    "\n",
    "When memory blocks get too long, they are automatically \"truncated\". By default, this just means they are removed from memory until there is room again. This can be customized with subclasses of memory blocks that implement their own truncation logic.\n",
    "\n",
    "- `priority=0`: This block will always be kept in memory.\n",
    "- `priority=1, 2, 3`, etc: This determines the order in which memory blocks are truncated when the memory exceeds the token limit, to help the overall short-term memory + long-term memory content be less than or equal to the token_limit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory.from_defaults(\n",
    "    session_id=\"my_session\",\n",
    "    token_limit=40000,\n",
    "    memory_blocks=[\n",
    "        static_memory_block,\n",
    "        facts_block,\n",
    "        vector_memory_block,\n",
    "    ],\n",
    "    insert_method=\"system\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the memory is used, the short-term memory will fill up. Once the short-term memory exceeds the `chat_history_token_ratio`, the oldest messages that fit into the `token_flush_size` will be flushed and sent to each memory block for processing.\n",
    "\n",
    "When memory is retrieved, the short-term and long-term memories are merged together. The Memory object will ensure that the short-term memory + long-term memory content is less than or equal to the token_limit. If it is longer, the `.truncate()` method will be called on the memory blocks, using the priority to determine the truncation order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = memory.get_all()\n",
    "\n",
    "for message in messages:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
