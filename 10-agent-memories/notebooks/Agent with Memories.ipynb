{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FunctionAgent with Memories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a FunctionAgent with memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "from llama_index.core.memory import Memory\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.workflow import Context\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "memory = Memory.from_defaults(\n",
    "    session_id=\"default_session\", \n",
    "    token_limit=40000\n",
    ")\n",
    "\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Usfeful for getting the weather for a given location.\"\"\"\n",
    "    return f\"The weather at {location} is very nice with not much rain.\"\n",
    "\n",
    "weather_tool = FunctionTool.from_defaults(\n",
    "    get_weather,\n",
    ")\n",
    "\n",
    "agent = FunctionAgent(llm=llm, tools=[weather_tool])\n",
    "ctx = Context(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in Tokyo is very nice with not much rain.\n"
     ]
    }
   ],
   "source": [
    "response = await agent.run(\"How is the weather in Tokyo?\", memory=memory)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm doing well, thank you! I'm ChatGPT, your AI assistant. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "response = await agent.run(\"How are you?  What is your name?\", memory=memory)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short-Term Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory.from_defaults(\n",
    "    session_id=\"my_session\",\n",
    "    token_limit=40000,\n",
    "    chat_history_token_ratio=0.7,\n",
    "    token_flush_size=3000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long-Term Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import (\n",
    "    StaticMemoryBlock,\n",
    "    FactExtractionMemoryBlock,\n",
    "    VectorMemoryBlock,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_memory_block = StaticMemoryBlock(\n",
    "        name=\"core_info\",\n",
    "        static_content=\"My name is MemAgent, and I live in San Francisco.  I am here to help ASDRP researhers\",\n",
    "        priority=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_extraction_memory_block = FactExtractionMemoryBlock(\n",
    "        name=\"extracted_info\",\n",
    "        llm=llm,\n",
    "        max_facts=50,\n",
    "        priority=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'vector_memory' already exists\n",
      "VectorMemoryBlock created successfully with Qdrant!\n"
     ]
    }
   ],
   "source": [
    "# First, start Qdrant locally using Docker (run this in terminal):\n",
    "# docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant\n",
    "\n",
    "from qdrant_client import AsyncQdrantClient, QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "def setup_qdrant_with_collection(collection_name=\"vector_memory\", vector_size=1536):\n",
    "    \"\"\"Setup Qdrant client and create collection if it doesn't exist\"\"\"\n",
    "    try:\n",
    "        q_client = QdrantClient(\n",
    "            url=\"http://localhost:6333\",\n",
    "        )\n",
    "        q_aclient = AsyncQdrantClient(\n",
    "            url=\"http://localhost:6333\",\n",
    "        )\n",
    "        \n",
    "        # Check if collection exists, if not create it\n",
    "        try:\n",
    "            collection_info = q_client.get_collection(collection_name)\n",
    "            print(f\"Collection '{collection_name}' already exists\")\n",
    "        except Exception: # Collection doesn't exist, create it\n",
    "            q_client.create_collection(\n",
    "                collection_name=collection_name,\n",
    "                vectors_config=models.VectorParams(\n",
    "                    size=vector_size,  # This should match your embedding dimension\n",
    "                    distance=models.Distance.COSINE\n",
    "                )\n",
    "            )\n",
    "            print(f\"Created collection '{collection_name}'\")\n",
    "        \n",
    "        # Create the vector store\n",
    "        vector_store = QdrantVectorStore(\n",
    "            collection_name=collection_name,\n",
    "            client=q_client,\n",
    "            aclient=q_aclient,\n",
    "            prefer_grpc=False,  # Use HTTP instead of gRPC for simplicity\n",
    "        )\n",
    "        \n",
    "        return vector_store, q_client\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Qdrant: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Setup Qdrant\n",
    "vector_store, qdrant_client = setup_qdrant_with_collection()\n",
    "\n",
    "if vector_store is not None:\n",
    "    vector_memory_block = VectorMemoryBlock(\n",
    "        name=\"vector_memory\",\n",
    "        vector_store=vector_store,\n",
    "        embed_model=embed_model,\n",
    "        priority=2,\n",
    "    )\n",
    "    print(\"VectorMemoryBlock created successfully with Qdrant!\")\n",
    "else:\n",
    "    print(\"Skipping VectorMemoryBlock creation due to Qdrant connection issues\")\n",
    "    vector_memory_block = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_term_memory_blocks = [\n",
    "    static_memory_block,\n",
    "    fact_extraction_memory_block,\n",
    "    vector_memory_block,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.memory import Memory, InsertMethod\n",
    "\n",
    "memory = Memory.from_defaults(\n",
    "    session_id=\"my_session\",\n",
    "    token_limit=40000,\n",
    "    chat_history_token_ratio=0.7,\n",
    "    token_flush_size=3000,\n",
    "    memory_blocks=long_term_memory_blocks,\n",
    "    insert_method=InsertMethod.SYSTEM\n",
    ")\n",
    "\n",
    "print(memory.get_all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in Tokyo is very nice with not much rain.\n",
      "[ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='How is the weather in Tokyo?')]), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_tJoUd4foVLAU17KoayZOY1Y3', 'function': {'arguments': '{\"location\":\"Tokyo\"}', 'name': 'get_weather'}, 'type': 'function'}]}, blocks=[TextBlock(block_type='text', text='')]), ChatMessage(role=<MessageRole.TOOL: 'tool'>, additional_kwargs={'tool_call_id': 'call_tJoUd4foVLAU17KoayZOY1Y3'}, blocks=[TextBlock(block_type='text', text='The weather at Tokyo is very nice with not much rain.')]), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='The weather in Tokyo is very nice with not much rain.')])]\n"
     ]
    }
   ],
   "source": [
    "response = await agent.run(\"How is the weather in Tokyo?\", memory=memory)\n",
    "print(response)\n",
    "\n",
    "print(memory.get_all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = memory.get_all()\n",
    "len(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> role: MessageRole.USER\n",
      "\tCONTENT: How is the weather in Tokyo?\n",
      "=> role: MessageRole.ASSISTANT\n",
      "\tTOOL: get_weather: {\"location\":\"Tokyo\"}\n",
      "=> role: MessageRole.TOOL\n",
      "\tCONTENT: The weather at Tokyo is very nice with not much rain.\n",
      "=> role: MessageRole.ASSISTANT\n",
      "\tCONTENT: The weather in Tokyo is very nice with not much rain.\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "for message in memory.get_all():\n",
    "    print(f\"=> role: {message.role}\")\n",
    "    if message.content:\n",
    "        print(f\"\\tCONTENT: {message.content}\")\n",
    "    if 'tool_calls' in message.additional_kwargs.keys():\n",
    "        for tool_call in message.additional_kwargs['tool_calls']:\n",
    "            print(f\"\\tTOOL: {tool_call['function']['name']}: {tool_call['function']['arguments']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
